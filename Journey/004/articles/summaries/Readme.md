# Feature Selection and EDA in Machine Learning
(How to Use Data Visualization to Guide Feature Selection)

<p align="center">
  <img src="https://github.com/akimwong/1_OnPremise/blob/main/Journey/004/articles/summaries/EdaAndFeatureSelectionTechniques1.jpg" width="900" height="600">
</p>

- In Machine Learning Lifecycle, feature selection is a critical process that selects a subset of input features that would be relevant to the prediction.
- Including irrelevant variables, especially those with bad data quality, can often contaminate the model output.
- Feature selection has following advantages:

1. `avoid the curse of dimensionality`, as some algorithms perform badly when high in dimensionality, e.g. general linear models, decision tree
2. `reduce computational cost and the complexity` that comes along with a large amount of data
3. `reduce overfitting` and the model is more likely to be generalized to new data
4. `increase the explainability` of models

## 1. Data Preprocessing

Before jumping into the feature selection, we should always load the dataset, perform data preprocessing and data transformation:

1. Load dataset and import libraries <br/>
2. Define the prediction <br/>
3. Examine missing data <br/>
4. Variable transformation (consists of encoding categorical variables and transforming all variables into the same scale (label encoder/min-max scaling)). <br/>

## 2. Exploratory Data Analysis (EDA)

Data visualization and EDA can be great complementary tools to feature selection process, and they can be applied in the following ways:

1. `Univariate Analysis`: Histogram and Bar Chart help to visualize the `distribution and variance of each variable`

- To obtain an overview of distribution, firstly let’s classify features in to categorical and numerical variables, then visualize categorical features using bar chart and numerical features using histogram. 
- Visualizing the distribution gives `suggestions on whether the data points are more dense or more spread out`, hence whether the variance is low or high. 
- `Low variance features` tend to `contribute less to the prediction` of outcome variable.

2. `Correlation Analysis`: Heatmap facilitates the `identification of highly correlated explanatory variables and reduce collinearity`

- `Some algorithms demands the absence of collinearity` in explanatory variables, including `logistic regression` which will be used for this exercise.
- Therefore, `eliminating highly correlated features is an essential step` to avoid this potential pitfall. 
- Correlation analysis with heatmap visualization highlights pairs of features with high correlation coefficient.

5. Bivariate Analysis: Box plot and Grouped bar chart `help to spot the dependency and relationship between explanatory variables` and response variable

- `Bivariate EDA` investigates the `relationship between each explanatory variable and the target variable`. 
- Categorical features and numerical variables are addressed using grouped bar chart and box plot respectively, and this exploration can further facilitate the statistical tests used in the filter methods, e.g. Chi-Square and ANOVA test.

#### Box plot is used as the visual representation of ANOVA analysis

- `Box plot displays the distributions of groups of numerical data` through their quantiles. 
- Each box shows how spread out the data is within group and putting boxes side by side indicates the difference among groups. 
- It is `aligned with ANOVA test` which also analyze the `degree of variance between-group compared to within-group`.  If the relative variability is large, then it may be an indication of that these features could contribute to predicting the labels. 

## Feature Selection

- In this article, we will mainly introduce two types of feature selection methods: filter method and wrapper method. 
- The fundamental difference is that:

1. `Filter method` evaluates the `feature importance based on statistical` tests such as Chi-Square, ANOVA etc, whereas 
2. `Wrapper method iteratively assessed the performance of subsets of features` based the performance of models generated by these features.

### 1. Filter Methods

<p align="center">
  <img src="https://github.com/akimwong/1_OnPremise/blob/main/Journey/004/articles/summaries/EdaAndFeatureSelectionTechniques2.png" width="600" height="100">
</p>


- Give a `score to each feature` by evaluating its relationship `with the dependent variable`. 
- For classification problems with categorical response variables, I am using these three major scoring functions: Chi-Square (score_func = chi2), ANOVA (score_func = f_classif), and Mutual Information (score_func = mutual_info_classif). 
- To create a feature selection model, we need: <br/>

A. The SelectKBest() function,  <br/>
B. Then specify which scoring functions to utilize and  <br/>
C. The how many variables to select. <br/>

-I would like to know how these two parameters, scoring function and the number of variables, would affect the accuracy of the model trained on the selected features.

#### 1.1. Firstly, to create the carry out the feature selection and examine the performance of the model built upon it, I define a feature_selection function with following steps:
- import required libraries
- create a feature selection model based on two parameters: score_function (e.g. chi square) and variable counts (e.g. ranging from 1 to all features)
- train a logistic regression model based on selected features only
- calculates the accuracy score

#### 1.2. Secondly, to test how score functions and variable counts would affect the model performance, 
- I iteratively passing different combinations of two parameters, “variable_counts” and “score_function”, using the following code.

#### Filter Methods with Data Visualization
- The result was generated in a data frame format and then use line chart to demonstrate how the accuracy progress as the number of selected features grows. As shown, except mutual information method, the accuracy score stabilizes around 0.88 after reaching 8 features (for the example of the article).
- Afterwards, let’s investigate what is the score of each feature based on various approaches. This time we will use bar charts to visualize the scores has been allocated to features according to chi-square, anova or mutual information.
- As you can see, `different approach scores the same feature differently`, but `some features always appear higher` up on the list. 

### 2. Wrapper Methods

<p align="center">
  <img src="https://github.com/akimwong/1_OnPremise/blob/main/Journey/004/articles/summaries/EdaAndFeatureSelectionTechniques3.png" width="600" height="150">
</p>

- Find the optimal subsets of features by `evaluating the performance of machine learning models trained` upon these features.
- It requires more computational power. 
- This article covers two main wrapper methods, `forward selection` and `backward elimination`.
- To perform forward selection and backward elimination, we need SequentialFeatureSelector() function which primarily requires four parameters: <br/>
A. `Model`: for classification problem, we can use Logistic Regression, KNN etc, and for regression problem, we can use linear regression etc <br/>
B. `k_features`: the number of features to be selected <br/>
C. `Forward`: determine whether it is forward selection or backward elimination <br/>
D. `Scoring`: evaluation metrics to determine the model performance, e.g. classification problem — accuracy, precision, recall etc; regression problem — p-value, R-squared etc <br/>

#### 2.1. Forward Selection
- Starts with no features in the model and incrementally adds one feature to the feature subset at a time. 
- During each iteration, the new feature is chosen based on the evaluation of the model trained by the feature subset.
- Since the machine learning model is wrapped within the feature selection algorithm, we need to specify a model as one of the input parameters. I choose Logistic Regression for this classification problem and accuracy as the evaluation metrics. 
- There is a slight difference in calculating the accuracy in the wrapper method compared to the filter method. Since we only fit the training set to the wrapper model, the accuracy score returned by the wrapper method itself is purely based on the training dataset. 
- Therefore, it is necessary to train an additional model on the selected features and further evaluated based on the test set.

#### 2.2. Backward Elimination
- It is just the opposite of the forward selection, starting with including all features to train the model. 
- Then, features are iteratively removed from the feature subset based on whether they contribute to the model performance. 
- Similarly, logistic regression and accuracy are used as the model and evaluation metrics correspondingly.

----------------------
# 5 Feature Selection Method from Scikit-Learn you should know
(Improve your performance by selecting only the most important features)

- Increasing the number of features would help the model to have a good prediction power, `but only until a certain point`. 
- This is what we called a `Curse of Dimensionality`, where the model's performance would increase with the higher number of features we used. 
- Still, `the performance will deteriorate when the feature number is past the peak`. 
- That is why we need to select only the features that are effectively capable of prediction.
- `Feature selection is similar to the dimensionality reduction technique`, where the aim is to reduce the number of the features, but fundamentally they are different. 
- The difference is that `feature selection selects features to keep or remove from the dataset`, whereas `dimensionality reduction creates a projection of the data resulting in entirely new input features`. 

## 1. Variance Threshold Feature Selection
- A feature with a higher variance means that the value within that feature varies or has a high cardinality.
- A variance Threshold is a simple approach to eliminating features based on our expected variance within each feature.
- Although, there are some down-side to the Variance Threshold method. 
- The Variance Threshold feature selection `only sees the input features (X) without considering any information from the dependent variable (y). It is only useful for eliminating features for Unsupervised Modelling` rather than Supervised Modelling.

## 2. Univariate Feature Selection with SelectKBest
- Univariate Feature Selection is a feature selection method `based on the univariate statistical test`, e,g: chi2, Pearson-correlation, and many more.
- The premise with SelectKBest is `combining the univariate statistical test with selecting the K-number of features` based on the statistical result between the X and y.


