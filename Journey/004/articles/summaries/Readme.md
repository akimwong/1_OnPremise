# Feature Selection and EDA in Machine Learning
(How to Use Data Visualization to Guide Feature Selection)

<p align="center">
  <img src="https://github.com/akimwong/1_OnPremise/blob/main/Journey/004/articles/summaries/EdaAndFeatureSelectionTechniques1.jpg" width="900" height="600">
</p>

- In Machine Learning Lifecycle, feature selection is a critical process that selects a subset of input features that would be relevant to the prediction.
- Including irrelevant variables, especially those with bad data quality, can often contaminate the model output.
- Feature selection has following advantages:

1. `avoid the curse of dimensionality`, as some algorithms perform badly when high in dimensionality, e.g. general linear models, decision tree
2. `reduce computational cost and the complexity` that comes along with a large amount of data
3. `reduce overfitting` and the model is more likely to be generalized to new data
4. `increase the explainability` of models

## 1. Data Preprocessing

Before jumping into the feature selection, we should always load the dataset, perform data preprocessing and data transformation:

1. Load dataset and import libraries <br/>
2. Define the prediction <br/>
3. Examine missing data <br/>
4. Variable transformation (consists of encoding categorical variables and transforming all variables into the same scale (label encoder/min-max scaling)). <br/>

## 2. Exploratory Data Analysis (EDA)

Data visualization and EDA can be great complementary tools to feature selection process, and they can be applied in the following ways:

1. `Univariate Analysis`: Histogram and Bar Chart help to visualize the `distribution and variance of each variable`

- To obtain an overview of distribution, firstly letâ€™s classify features in to categorical and numerical variables, then visualize categorical features using bar chart and numerical features using histogram. 
- Visualizing the distribution gives `suggestions on whether the data points are more dense or more spread out`, hence whether the variance is low or high. 
- `Low variance features` tend to `contribute less to the prediction` of outcome variable.

2. Correlation Analysis: Heatmap facilitates the identification of highly correlated explanatory variables and reduce collinearity

- Some algorithms demands the absence of collinearity in explanatory variables, including logistic regression which will be used for this exercise.
- Therefore, eliminating highly correlated features is an essential step to avoid this potential pitfall. 
- Correlation analysis with heatmap visualization highlights pairs of features with high correlation coefficient.

5. Bivariate Analysis: Box plot and Grouped bar chart help to spot the dependency and relationship between explanatory variables and response variable

- Bivariate EDA investigates the relationship between each explanatory variable and the target variable. 
- Categorical features and numerical variables are addressed using grouped bar chart and box plot respectively, and this exploration can further facilitate the statistical tests used in the filter methods, e.g. Chi-Square and ANOVA test.

#### Box plot is used as the visual representation of ANOVA analysis

- Box plot displays the distributions of groups of numerical data through their quantiles. 
- Each box shows how spread out the data is within group and putting boxes side by side indicates the difference among groups. 
- It is aligned with ANOVA test which also analyze the degree of variance between-group compared to within-group. 

## Feature Selection

- In this article, we will mainly introduce two types of feature selection methods: filter method and wrapper method. 
- The fundamental difference is that:

1. Filter method evaluates the feature importance based on statistical tests such as Chi-Square, ANOVA etc, whereas 
2. Wrapper method iteratively assessed the performance of subsets of features based the performance of models generated by these features.

### 1. Filter Methods

- Give a score to each feature by evaluating its relationship with the dependent variable. 
- For classification problems with categorical response variables, I am using these three major scoring functions: Chi-Square (score_func = chi2), ANOVA (score_func = f_classif), and Mutual Information (score_func = mutual_info_classif). 
- To create a feature selection model, we need the SelectKBest() function, then specify which scoring functions to utilize and the how many variables to select.

